{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Too Few Loci Problem\n",
    "\n",
    "20161208\n",
    "\n",
    "I only retained about 6k loci in my run, after Bowtie and Blast filtering, and because I'm using Pacific cod just like Mary, I should be expecting much more than that. Mary has ~ 16k loci. Dan says I should expect 20-40k loci.\n",
    "\n",
    "I want to know whether it was \n",
    "<br>(1) ``populations`` parameters that led to low retained loci NO\n",
    "<br>(2) the individuals I used in ``cstacks`` that led to low retained loci NOPE\n",
    "<br>(3) or the parameters in ``cstacks`` that led to low retained loci NOPE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking ``cstacks``\n",
    "\n",
    "I opened my catalog.tags file for batch 2 (the one where I used 10 individuals with a lot of reads to make the catalog) and there are 53885 tags. That seems like a ton of tags, which suggests to me that the problem might be after ``cstacks``. \n",
    "\n",
    "However, after futzing around with the ``populations`` parameters, I realize that none of them really bring me up to the 15k ish loci that Mary is getting for Pacific cod. This makes me think it is in ``cstacks``. My thought is that I could have used unideal individuals for making the catalog. I should make sure that the individuals I used represent all of the populations.\n",
    "\n",
    "I went back and reran ``cstacks`` on two individuals from each of my 5 cohorots (the ones with the most reads) and now the number of tags is 53034, so almost the same as previously. It could be that they're better loci that won't get filtered out later, I guess. Worth following through to ``populations`` to find out. \n",
    "\n",
    "After ``populations`` batch 3, I only hat 6813 loci, which isn't much different from when I used other individuals to make the catalog. Sigh! After chatting with Eleni, I think it could be that I'm using a few individuals that had very low quality reads, meaning there were very few loci called for certain individuals. I'm going to see how many tags remain per individual after ustacks with a ``grep`` command Eleni wrote, below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ``populations`` parameters\n",
    "\n",
    "The following parameters will affect how  many loci are retained. \n",
    "\n",
    "<br>-m [int] — specify a minimum stack depth required for individuals at a locus.\n",
    "<br>-r [float] — minimum percentage of individuals in a population required to process a locus for that population.\n",
    "<br>-p [int] — minimum number of populations a locus must be present in to process a locus.\n",
    "\n",
    "Count how many loci were retained after ``populations``, then change parameters and rerun ``populations`` and compare.\n",
    "\n",
    "When I ran the program for batch_2, it looked like this:\n",
    "\n",
    "```\n",
    "!populations -b 2 -P ustacks_out -M popmap1.txt -t 10 -r 0.50 -p 2 -m 5 --genepop\n",
    "```\n",
    "\n",
    "#### Testing -p\n",
    "I'm thinking that this is the issue because I only have 3 populations in this subset (batch_2) of my data, and one of the populations only had 2 individuals. So I'm going to test -p of 1 and -p of 2. Mary usually uses -p of 1 or 2, but she also has 7 populations.\n",
    "\n",
    "My first run of populations (like the line of code above) produced 6165 unique loci\n",
    "\n",
    "For this test, -p of 1, and the rest the same as in batch_2: this produced 6531 unique loci\n",
    "\n",
    "#### Testing -r\n",
    "\n",
    "Try .25 versus .5\n",
    "\n",
    "My first run (like the line of code above) with a -r of .5 produced 6165 unique loci.\n",
    "\n",
    "For this test, -r of .25, the rest as in batch_2: this produced 7025 unique loci.\n",
    "\n",
    "#### Testing -m\n",
    "A -m of 5 is already on the low side, as most people in our lab use a -m of 10. May be worth trying 7 and 10 just to see how this affects number of retained loci.\n",
    "\n",
    "My first run (like the line of code above) with a -m of 5 produced 6165 unique loci.\n",
    "For this test, -m of 7, the rest as in batch_2: this produced 6992 unique loci.\n",
    "For this test, -m of 10, the rest as in batch_2: this produced 6897 unique loci.\n",
    "\n",
    "### The problem is probably not in the ``populations`` parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Volumes/Time Machine Backups/Cod-Time-Series-Data\n"
     ]
    }
   ],
   "source": [
    "cd /Volumes/Time\\ Machine\\ Backups/Cod-Time-Series-Data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fst kernel smoothing: off\r\n",
      "Bootstrap resampling: off\r\n",
      "Percent samples limit per population: 0.5\r\n",
      "Locus Population limit: 1\r\n",
      "Minimum stack depth: 5\r\n",
      "Log liklihood filtering: off; threshold: 0\r\n",
      "Minor allele frequency cutoff: 0\r\n",
      "Maximum observed heterozygosity cutoff: 1\r\n",
      "Applying Fst correction: none.\r\n",
      "Parsing population map...\r\n",
      "The population map contained 26 samples, 3 population(s), 1 group(s).\r\n",
      "Reading the catalog...\r\n",
      " Unable to open 'ustacks_out/batch_100.catalog'\r\n",
      "Unable to load the catalog 'ustacks_out/batch_100.catalog'\r\n"
     ]
    }
   ],
   "source": [
    "!populations -b 2 -P ustacks_out -M popmap1.txt -t 10 -r 0.50 -p 1 -m 5 --genepop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking if low quality individuals are causing loci loss\n",
    "\n",
    "Eleni uses this code to count how many tags were called in each individual after ``pstacks``, which is interchangeable with ``ustacks``.\n",
    "\n",
    "<br>\n",
    "```\n",
    "grep --count --with-filename consensus *.tags.tsv > output_file.txt\n",
    "```\n",
    "\n",
    "First, I need to put all the individual tag files into their own directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/natalielowell/Desktop/b3_ind_tags\n"
     ]
    }
   ],
   "source": [
    "cd /Users/natalielowell/Desktop/b3_ind_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\r\n"
     ]
    }
   ],
   "source": [
    "!zgrep --count --with-filename consensus *.tags.tsv.gz > output_file.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Volumes/Time Machine Backups/Cod-Time-Series-Data\n"
     ]
    }
   ],
   "source": [
    "cd /Volumes/Time\\ Machine\\ Backups/Cod-Time-Series-Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I wrote a short script to get the year and retained loci tab delimited, so it's easier to stick in excel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2005\t38259\r\n",
      "2005\t40025\r\n",
      "2005\t30130\r\n",
      "2005\t36278\r\n",
      "2005\t20263\r\n",
      "2005\t4494\r\n",
      "2005\t22247\r\n",
      "2005\t39554\r\n",
      "2005\t38890\r\n",
      "2005\t48108\r\n",
      "2005\t40044\r\n",
      "2005\t37589\r\n",
      "2005\t39969\r\n",
      "2005\t39194\r\n",
      "2005\t45666\r\n",
      "2005\t47839\r\n",
      "2005\t40681\r\n",
      "2005\t39775\r\n",
      "2005\t38963\r\n",
      "2009\t12910\r\n",
      "2009\t28080\r\n",
      "2009\t374\r\n",
      "2009\t377\r\n",
      "2009\t146\r\n",
      "2009\t802\r\n",
      "2009\t13679\r\n",
      "2009\t3878\r\n",
      "2009\t1196\r\n",
      "2009\t1698\r\n",
      "2009\t23\r\n",
      "2009\t339\r\n",
      "2009\t12673\r\n",
      "2009\t1084\r\n",
      "2009\t3797\r\n",
      "2009\t16040\r\n",
      "2009\t33531\r\n",
      "2009\t753\r\n",
      "2009\t854\r\n",
      "2009\t1210\r\n",
      "2009\t2858\r\n",
      "2010\t40480\r\n",
      "2010\t39684\r\n",
      "2010\t39063\r\n",
      "2010\t40584\r\n",
      "2010\t40908\r\n",
      "2010\t40388\r\n",
      "2010\t39891\r\n",
      "2010\t40812\r\n",
      "2010\t38927\r\n",
      "2010\t40377\r\n",
      "2010\t40066\r\n",
      "2010\t40639\r\n",
      "2010\t40615\r\n",
      "2010\t40498\r\n",
      "2010\t42378\r\n",
      "2010\t42730\r\n",
      "2010\t41699\r\n",
      "2010\t41511\r\n",
      "2010\t40724\r\n",
      "2010\t48933\r\n",
      "2010\t44154\r\n",
      "2010\t40374\r\n",
      "2010\t41742\r\n",
      "2010\t41231\r\n",
      "2010\t41557\r\n",
      "2010\t40273\r\n",
      "2010\t39145\r\n",
      "2010\t35429\r\n",
      "2010\t39106\r\n",
      "2010\t38087\r\n",
      "2010\t35731\r\n",
      "2010\t36480\r\n",
      "2010\t44401\r\n",
      "2010\t40536\r\n",
      "2010\t40146\r\n",
      "2010\t42326\r\n",
      "2010\t42119\r\n",
      "2010\t40447\r\n",
      "2014\t39933\r\n",
      "2014\t38234\r\n",
      "2014\t39741\r\n",
      "2014\t37365\r\n",
      "2014\t41879\r\n",
      "2014\t37765\r\n",
      "2014\t37625\r\n",
      "2014\t37605\r\n",
      "2014\t36696\r\n",
      "2014\t38249\r\n",
      "2014\t35202\r\n",
      "2014\t38614\r\n",
      "2015\t38772\r\n",
      "2015\t36690\r\n",
      "2015\t38257\r\n",
      "2015\t39333\r\n",
      "2015\t37653\r\n",
      "2015\t39069\r\n",
      "2015\t37080\r\n",
      "2015\t38404\r\n",
      "2015\t37773\r\n",
      "2015\t37763\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!python tag_count_ind_forexcel.py output_file.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I plotted retained loci per year in Excel, see below.\n",
    "<br>\n",
    "![image](https://github.com/nclowell/FISH546/blob/master/Cod-Time-Series-Project/Notebooks/images_for_notebooks/Screen%20Shot%202016-12-12%20at%209.43.28%20AM.png?raw=true)\n",
    "<br>\n",
    "Looking back at the metadata for these samples, the 2009 ones were particularly degraded, so this was probably to be expected. Eleni makes an arbitrary cut off for which samples she will retain, so I'm going to try to cut-offs just to see what the effect is on the number of retained loci at the end. My cut-offs will be 20,000 and 30,000 loci. \n",
    "\n",
    "My hypothesis is that these degraded samples are causing the loss of retained loci, so I expect my cut-offs will increase the number of retained loci."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Volumes/Time Machine Backups/Cod-Time-Series-Data\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fst kernel smoothing: off\n",
      "Bootstrap resampling: off\n",
      "Percent samples limit per population: 0.5\n",
      "Locus Population limit: 1\n",
      "Minimum stack depth: 10\n",
      "Log liklihood filtering: off; threshold: 0\n",
      "Minor allele frequency cutoff: 0\n",
      "Maximum observed heterozygosity cutoff: 1\n",
      "Applying Fst correction: none.\n",
      "Parsing population map...\n",
      "The population map contained 102 samples, 5 population(s), 1 group(s).\n",
      "Reading the catalog...\n",
      "  Parsing test_20k_cutoff/batch_3.catalog.tags.tsv\n",
      "  Parsing test_20k_cutoff/batch_3.catalog.snps.tsv.gz\n",
      "  Parsing test_20k_cutoff/batch_3.catalog.alleles.tsv.gz\n",
      "Reading matches to the catalog...\n",
      "  Parsing test_20k_cutoff/2005_297_1.matches.tsv.gz\n",
      "  Parsing test_20k_cutoff/2005_298_1.matches.tsv.gz\n",
      "  Parsing test_20k_cutoff/2005_299_1.matches.tsv.gz\n",
      "  Parsing test_20k_cutoff/2005_384_1.matches.tsv.gz\n",
      "  Parsing test_20k_cutoff/2005_385_1.matches.tsv.gz\n",
      "  Parsing test_20k_cutoff/2005_387_1.matches.tsv.gz\n",
      "  Parsing test_20k_cutoff/2005_388_1.matches.tsv.gz\n",
      "  Parsing test_20k_cutoff/2005_389_1.matches.tsv.gz\n",
      "  Parsing test_20k_cutoff/2005_457_1.matches.tsv.gz\n",
      "  Parsing test_20k_cutoff/2005_459_1.matches.tsv.gz\n",
      "  Parsing test_20k_cutoff/2005_460_1.matches.tsv.gz\n",
      "  Parsing test_20k_cutoff/2005_461_1.matches.tsv.gz\n",
      "  Parsing test_20k_cutoff/2005_462_1.matches.tsv.gz\n",
      "  Parsing test_20k_cutoff/2005_463_1.matches.tsv.gz\n",
      "  Parsing test_20k_cutoff/2005_464_1.matches.tsv.gz\n",
      "  Parsing test_20k_cutoff/2005_465_1.matches.tsv.gz\n",
      "  Parsing test_20k_cutoff/2005_466_1.matches.tsv.gz\n",
      "  Parsing test_20k_cutoff/2005_467_1.matches.tsv.gz\n",
      "  Parsing test_20k_cutoff/2005_495_1.matches.tsv.gz\n",
      "  Parsing test_20k_cutoff/2005_559_1.matches.tsv.gz\n",
      "Warning: Absent or malformed matches file '2009_123_1.matches.tsv(.gz)', excluding this sample from population analysis.\n",
      "  Parsing test_20k_cutoff/2009_124_1.matches.tsv.gz\n",
      "Warning: Absent or malformed matches file '2009_127_1.matches.tsv(.gz)', excluding this sample from population analysis.\n",
      "Warning: Absent or malformed matches file '2009_128_1.matches.tsv(.gz)', excluding this sample from population analysis.\n",
      "Warning: Absent or malformed matches file '2009_129_1.matches.tsv(.gz)', excluding this sample from population analysis.\n",
      "Warning: Absent or malformed matches file '2009_131_1.matches.tsv(.gz)', excluding this sample from population analysis.\n",
      "Warning: Absent or malformed matches file '2009_135_1.matches.tsv(.gz)', excluding this sample from population analysis.\n",
      "Warning: Absent or malformed matches file '2009_141_1.matches.tsv(.gz)', excluding this sample from population analysis.\n",
      "Warning: Absent or malformed matches file '2009_147_1.matches.tsv(.gz)', excluding this sample from population analysis.\n",
      "Warning: Absent or malformed matches file '2009_164_1.matches.tsv(.gz)', excluding this sample from population analysis.\n",
      "Warning: Absent or malformed matches file '2009_169_1.matches.tsv(.gz)', excluding this sample from population analysis.\n",
      "Warning: Absent or malformed matches file '2009_170_1.matches.tsv(.gz)', excluding this sample from population analysis.\n",
      "Warning: Absent or malformed matches file '2009_171_1.matches.tsv(.gz)', excluding this sample from population analysis.\n",
      "Warning: Absent or malformed matches file '2009_172_1.matches.tsv(.gz)', excluding this sample from population analysis.\n",
      "Warning: Absent or malformed matches file '2009_173_1.matches.tsv(.gz)', excluding this sample from population analysis.\n",
      "Warning: Absent or malformed matches file '2009_174_1.matches.tsv(.gz)', excluding this sample from population analysis.\n",
      "  Parsing test_20k_cutoff/2009_176_1.matches.tsv.gz\n",
      "Warning: Absent or malformed matches file '2009_178_1.matches.tsv(.gz)', excluding this sample from population analysis.\n",
      "Warning: Absent or malformed matches file '2009_532_1.matches.tsv(.gz)', excluding this sample from population analysis.\n",
      "Warning: Absent or malformed matches file '2009_536_1.matches.tsv(.gz)', excluding this sample from population analysis.\n",
      "Warning: Absent or malformed matches file '2009_537_1.matches.tsv(.gz)', excluding this sample from population analysis.\n",
      "  Parsing test_20k_cutoff/2010_114_1.matches.tsv.gz\n",
      "  Parsing test_20k_cutoff/2010_115_1.matches.tsv.gz\n",
      "  Parsing test_20k_cutoff/2010_116_1.matches.tsv.gz\n",
      "  Parsing test_20k_cutoff/2010_117_1.matches.tsv.gz\n",
      "  Parsing test_20k_cutoff/2010_118_1.matches.tsv.gz\n",
      "  Parsing test_20k_cutoff/2010_119_1.matches.tsv.gz\n",
      "  Parsing test_20k_cutoff/2010_123_1.matches.tsv.gz\n",
      "  Parsing test_20k_cutoff/2010_127_1.matches.tsv.gz\n",
      "  Parsing test_20k_cutoff/2010_184_1.matches.tsv.gz\n",
      "  Parsing test_20k_cutoff/2010_185_1.matches.tsv.gz\n",
      "  Parsing test_20k_cutoff/2010_187_1.matches.tsv.gz\n",
      "  Parsing test_20k_cutoff/2010_188_1.matches.tsv.gz\n",
      "  Parsing test_20k_cutoff/2010_189_1.matches.tsv.gz\n",
      "  Parsing test_20k_cutoff/2010_211_1.matches.tsv.gz\n",
      "  Parsing test_20k_cutoff/2010_212_1.matches.tsv.gz\n",
      "  Parsing test_20k_cutoff/2010_213_1.matches.tsv.gz\n",
      "  Parsing test_20k_cutoff/2010_215_1.matches.tsv.gz\n",
      "  Parsing test_20k_cutoff/2010_216_1.matches.tsv.gz\n",
      "  Parsing test_20k_cutoff/2010_218_1.matches.tsv.gz\n",
      "  Parsing test_20k_cutoff/2010_219_1.matches.tsv.gz\n",
      "  Parsing test_20k_cutoff/2010_220_1.matches.tsv.gz\n",
      "  Parsing test_20k_cutoff/2010_221_1.matches.tsv.gz\n",
      "  Parsing test_20k_cutoff/2010_222_1.matches.tsv.gz\n",
      "  Parsing test_20k_cutoff/2010_240_1.matches.tsv.gz\n",
      "  Parsing test_20k_cutoff/2010_242_1.matches.tsv.gz\n",
      "  Parsing test_20k_cutoff/2010_243_1.matches.tsv.gz\n",
      "  Parsing test_20k_cutoff/2010_244_1.matches.tsv.gz\n",
      "  Parsing test_20k_cutoff/2010_246_1.matches.tsv.gz\n",
      "  Parsing test_20k_cutoff/2010_247_1.matches.tsv.gz\n",
      "  Parsing test_20k_cutoff/2010_250_1.matches.tsv.gz\n",
      "  Parsing test_20k_cutoff/2010_251_1.matches.tsv.gz\n",
      "  Parsing test_20k_cutoff/2010_266_1.matches.tsv.gz\n",
      "  Parsing test_20k_cutoff/2010_564_1.matches.tsv.gz\n",
      "  Parsing test_20k_cutoff/2010_565_1.matches.tsv.gz\n",
      "  Parsing test_20k_cutoff/2010_568_1.matches.tsv.gz\n",
      "  Parsing test_20k_cutoff/2010_569_1.matches.tsv.gz\n",
      "  Parsing test_20k_cutoff/2010_572_1.matches.tsv.gz\n",
      "  Parsing test_20k_cutoff/2010_858_1.matches.tsv.gz\n",
      "  Parsing test_20k_cutoff/2014_128_1.matches.tsv.gz\n",
      "  Parsing test_20k_cutoff/2014_129_1.matches.tsv.gz\n",
      "  Parsing test_20k_cutoff/2014_130_1.matches.tsv.gz\n",
      "Warning: Absent or malformed matches file '2014_174_1.matches.tsv(.gz)', excluding this sample from population analysis.\n",
      "  Parsing test_20k_cutoff/2014_203_1.matches.tsv.gz\n",
      "  Parsing test_20k_cutoff/2014_204_1.matches.tsv.gz\n",
      "  Parsing test_20k_cutoff/2014_205_1.matches.tsv.gz\n",
      "  Parsing test_20k_cutoff/2014_207_1.matches.tsv.gz\n",
      "  Parsing test_20k_cutoff/2014_208_1.matches.tsv.gz\n",
      "  Parsing test_20k_cutoff/2014_223_1.matches.tsv.gz\n",
      "  Parsing test_20k_cutoff/2014_224_1.matches.tsv.gz\n",
      "  Parsing test_20k_cutoff/2014_232_1.matches.tsv.gz\n",
      "Warning: Absent or malformed matches file '2015_218_1.matches.tsv(.gz)', excluding this sample from population analysis.\n",
      "  Parsing test_20k_cutoff/2015_016_1.matches.tsv.gz\n",
      "  Parsing test_20k_cutoff/2015_100_1.matches.tsv.gz\n",
      "  Parsing test_20k_cutoff/2015_101_1.matches.tsv.gz\n",
      "  Parsing test_20k_cutoff/2015_102_1.matches.tsv.gz\n",
      "  Parsing test_20k_cutoff/2015_103_1.matches.tsv.gz\n",
      "  Parsing test_20k_cutoff/2015_105_1.matches.tsv.gz\n",
      "  Parsing test_20k_cutoff/2015_138_1.matches.tsv.gz\n",
      "  Parsing test_20k_cutoff/2015_139_1.matches.tsv.gz\n",
      "  Parsing test_20k_cutoff/2015_140_1.matches.tsv.gz\n",
      "  Parsing test_20k_cutoff/2015_141_1.matches.tsv.gz\n",
      "Catalog is not reference aligned, arbitrarily ordering catalog loci.\n",
      "Working on 81 samples.\n",
      "Working on 5 population(s):\n",
      "    2005: 2005_297_1, 2005_298_1, 2005_299_1, 2005_384_1, 2005_385_1, 2005_387_1, 2005_388_1, 2005_389_1, 2005_457_1, 2005_459_1, 2005_460_1, 2005_461_1, 2005_462_1, 2005_463_1, 2005_464_1, 2005_465_1, 2005_466_1, 2005_467_1, 2005_495_1, 2005_559_1\n",
      "    2009: 2009_124_1, 2009_176_1\n",
      "    2010: 2010_114_1, 2010_115_1, 2010_116_1, 2010_117_1, 2010_118_1, 2010_119_1, 2010_123_1, 2010_127_1, 2010_184_1, 2010_185_1, 2010_187_1, 2010_188_1, 2010_189_1, 2010_211_1, 2010_212_1, 2010_213_1, 2010_215_1, 2010_216_1, 2010_218_1, 2010_219_1, 2010_220_1, 2010_221_1, 2010_222_1, 2010_240_1, 2010_242_1, 2010_243_1, 2010_244_1, 2010_246_1, 2010_247_1, 2010_250_1, 2010_251_1, 2010_266_1, 2010_564_1, 2010_565_1, 2010_568_1, 2010_569_1, 2010_572_1, 2010_858_1\n",
      "    2014: 2014_128_1, 2014_129_1, 2014_130_1, 2014_203_1, 2014_204_1, 2014_205_1, 2014_207_1, 2014_208_1, 2014_223_1, 2014_224_1, 2014_232_1\n",
      "    2015: 2015_016_1, 2015_100_1, 2015_101_1, 2015_102_1, 2015_103_1, 2015_105_1, 2015_138_1, 2015_139_1, 2015_140_1, 2015_141_1\n",
      "Working on 1 group(s) of populations:\n",
      "    defaultgrp: 2005, 2009, 2010, 2014, 2015\n",
      "Populating observed haplotypes for 81 samples, 53033 loci.\n",
      "Removed 104415 samples from loci that are below the minimum stack depth of 10x\n",
      "Removing 14279 loci that did not pass sample/population constraints... retained 38754 loci.\n",
      "Loading model outputs for 81 samples, 38754 loci.\n",
      "  Parsing test_20k_cutoff/2005_297_1.models.tsv.gz\n",
      "  Parsing test_20k_cutoff/2005_298_1.models.tsv.gz\n",
      "  Parsing test_20k_cutoff/2005_299_1.models.tsv.gz\n",
      "  Parsing test_20k_cutoff/2005_384_1.models.tsv.gz\n",
      "  Parsing test_20k_cutoff/2005_385_1.models.tsv.gz\n",
      "  Parsing test_20k_cutoff/2005_387_1.models.tsv.gz\n",
      "  Parsing test_20k_cutoff/2005_388_1.models.tsv.gz\n",
      "  Parsing test_20k_cutoff/2005_389_1.models.tsv.gz\n",
      "  Parsing test_20k_cutoff/2005_457_1.models.tsv.gz\n",
      "  Parsing test_20k_cutoff/2005_459_1.models.tsv.gz\n",
      "  Parsing test_20k_cutoff/2005_460_1.models.tsv.gz\n",
      "  Parsing test_20k_cutoff/2005_461_1.models.tsv.gz\n",
      "  Parsing test_20k_cutoff/2005_462_1.models.tsv.gz\n",
      "  Parsing test_20k_cutoff/2005_463_1.models.tsv.gz\n",
      "  Parsing test_20k_cutoff/2005_464_1.models.tsv.gz\n",
      "  Parsing test_20k_cutoff/2005_465_1.models.tsv.gz\n",
      "  Parsing test_20k_cutoff/2005_466_1.models.tsv.gz\n",
      "  Parsing test_20k_cutoff/2005_467_1.models.tsv.gz\n",
      "  Parsing test_20k_cutoff/2005_495_1.models.tsv.gz\n",
      "  Parsing test_20k_cutoff/2005_559_1.models.tsv.gz\n",
      "  Parsing test_20k_cutoff/2009_124_1.models.tsv.gz\n",
      "  Parsing test_20k_cutoff/2009_176_1.models.tsv.gz\n",
      "  Parsing test_20k_cutoff/2010_114_1.models.tsv.gz\n",
      "  Parsing test_20k_cutoff/2010_115_1.models.tsv.gz\n",
      "  Parsing test_20k_cutoff/2010_116_1.models.tsv.gz\n",
      "  Parsing test_20k_cutoff/2010_117_1.models.tsv.gz\n",
      "  Parsing test_20k_cutoff/2010_118_1.models.tsv.gz\n",
      "  Parsing test_20k_cutoff/2010_119_1.models.tsv.gz\n",
      "  Parsing test_20k_cutoff/2010_123_1.models.tsv.gz\n",
      "  Parsing test_20k_cutoff/2010_127_1.models.tsv.gz\n",
      "  Parsing test_20k_cutoff/2010_184_1.models.tsv.gz\n",
      "  Parsing test_20k_cutoff/2010_185_1.models.tsv.gz\n",
      "  Parsing test_20k_cutoff/2010_187_1.models.tsv.gz\n",
      "  Parsing test_20k_cutoff/2010_188_1.models.tsv.gz\n",
      "  Parsing test_20k_cutoff/2010_189_1.models.tsv.gz\n",
      "  Parsing test_20k_cutoff/2010_211_1.models.tsv.gz\n",
      "  Parsing test_20k_cutoff/2010_212_1.models.tsv.gz\n",
      "  Parsing test_20k_cutoff/2010_213_1.models.tsv.gz\n",
      "  Parsing test_20k_cutoff/2010_215_1.models.tsv.gz\n",
      "  Parsing test_20k_cutoff/2010_216_1.models.tsv.gz\n",
      "  Parsing test_20k_cutoff/2010_218_1.models.tsv.gz\n",
      "  Parsing test_20k_cutoff/2010_219_1.models.tsv.gz\n",
      "  Parsing test_20k_cutoff/2010_220_1.models.tsv.gz\n",
      "  Parsing test_20k_cutoff/2010_221_1.models.tsv.gz\n",
      "  Parsing test_20k_cutoff/2010_222_1.models.tsv.gz\n",
      "  Parsing test_20k_cutoff/2010_240_1.models.tsv.gz\n",
      "  Parsing test_20k_cutoff/2010_242_1.models.tsv.gz\n",
      "  Parsing test_20k_cutoff/2010_243_1.models.tsv.gz\n",
      "  Parsing test_20k_cutoff/2010_244_1.models.tsv.gz\n",
      "  Parsing test_20k_cutoff/2010_246_1.models.tsv.gz\n",
      "  Parsing test_20k_cutoff/2010_247_1.models.tsv.gz\n",
      "  Parsing test_20k_cutoff/2010_250_1.models.tsv.gz\n",
      "  Parsing test_20k_cutoff/2010_251_1.models.tsv.gz\n",
      "  Parsing test_20k_cutoff/2010_266_1.models.tsv.gz\n",
      "  Parsing test_20k_cutoff/2010_564_1.models.tsv.gz\n",
      "  Parsing test_20k_cutoff/2010_565_1.models.tsv.gz\n",
      "  Parsing test_20k_cutoff/2010_568_1.models.tsv.gz\n",
      "  Parsing test_20k_cutoff/2010_569_1.models.tsv.gz\n",
      "  Parsing test_20k_cutoff/2010_572_1.models.tsv.gz\n",
      "  Parsing test_20k_cutoff/2010_858_1.models.tsv.gz\n",
      "  Parsing test_20k_cutoff/2014_128_1.models.tsv.gz\n",
      "  Parsing test_20k_cutoff/2014_129_1.models.tsv.gz\n",
      "  Parsing test_20k_cutoff/2014_130_1.models.tsv.gz\n",
      "  Parsing test_20k_cutoff/2014_203_1.models.tsv.gz\n",
      "  Parsing test_20k_cutoff/2014_204_1.models.tsv.gz\n",
      "  Parsing test_20k_cutoff/2014_205_1.models.tsv.gz\n",
      "  Parsing test_20k_cutoff/2014_207_1.models.tsv.gz\n",
      "  Parsing test_20k_cutoff/2014_208_1.models.tsv.gz\n",
      "  Parsing test_20k_cutoff/2014_223_1.models.tsv.gz\n",
      "  Parsing test_20k_cutoff/2014_224_1.models.tsv.gz\n",
      "  Parsing test_20k_cutoff/2014_232_1.models.tsv.gz\n",
      "  Parsing test_20k_cutoff/2015_016_1.models.tsv.gz\n",
      "  Parsing test_20k_cutoff/2015_100_1.models.tsv.gz\n",
      "  Parsing test_20k_cutoff/2015_101_1.models.tsv.gz\n",
      "  Parsing test_20k_cutoff/2015_102_1.models.tsv.gz\n",
      "  Parsing test_20k_cutoff/2015_103_1.models.tsv.gz\n",
      "  Parsing test_20k_cutoff/2015_105_1.models.tsv.gz\n",
      "  Parsing test_20k_cutoff/2015_138_1.models.tsv.gz\n",
      "  Parsing test_20k_cutoff/2015_139_1.models.tsv.gz\n",
      "  Parsing test_20k_cutoff/2015_140_1.models.tsv.gz\n",
      "  Parsing test_20k_cutoff/2015_141_1.models.tsv.gz\n",
      "Generating nucleotide-level summary statistics for population '2005'\n",
      "Population '2005' contained 52 incompatible loci -- more than two alleles present.\n",
      "Generating nucleotide-level summary statistics for population '2009'\n",
      "Population '2009' contained 4 incompatible loci -- more than two alleles present.\n",
      "Generating nucleotide-level summary statistics for population '2010'\n",
      "Population '2010' contained 82 incompatible loci -- more than two alleles present.\n",
      "Generating nucleotide-level summary statistics for population '2014'\n",
      "Population '2014' contained 40 incompatible loci -- more than two alleles present.\n",
      "Generating nucleotide-level summary statistics for population '2015'\n",
      "Population '2015' contained 54 incompatible loci -- more than two alleles present.\n",
      "Tallying loci across populations...done.\n",
      "Pruned 247 variant sites due to filter constraints (more with --verbose).\n",
      "Removing 2381 additional loci for which all variant sites were filtered... retained 36373 loci.\n",
      "Regenerating nucleotide-level summary statistics for population '2005'\n",
      "Population '2005' contained 0 incompatible loci -- more than two alleles present.\n",
      "Regenerating nucleotide-level summary statistics for population '2009'\n",
      "Population '2009' contained 0 incompatible loci -- more than two alleles present.\n",
      "Regenerating nucleotide-level summary statistics for population '2010'\n",
      "Population '2010' contained 0 incompatible loci -- more than two alleles present.\n",
      "Regenerating nucleotide-level summary statistics for population '2014'\n",
      "Population '2014' contained 0 incompatible loci -- more than two alleles present.\n",
      "Regenerating nucleotide-level summary statistics for population '2015'\n",
      "Population '2015' contained 0 incompatible loci -- more than two alleles present.\n",
      "Re-tallying loci across populations...done.\n",
      "Generating haplotype-level summary statistics for population '2005'\n",
      "Generating haplotype-level summary statistics for population '2009'\n",
      "Generating haplotype-level summary statistics for population '2010'\n",
      "Generating haplotype-level summary statistics for population '2014'\n",
      "Generating haplotype-level summary statistics for population '2015'\n",
      "Writing 36373 loci to summary statistics file, 'test_20k_cutoff/batch_3.sumstats.tsv'\n",
      "Writing 36373 loci to observed haplotype file, 'test_20k_cutoff/batch_3.haplotypes.tsv'\n",
      "Writing population data to GenePop file 'test_20k_cutoff/batch_3.genepop'\n",
      "Populations is done.\n"
     ]
    }
   ],
   "source": [
    "!populations -b 3 -P test_20k_cutoff -M popmap_b3.txt -t 10 -r 0.50 -p 1 -m 10 --genepop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Volumes/Time Machine Backups/Cod-Time-Series-Data/scripts\n"
     ]
    }
   ],
   "source": [
    "cd /Volumes/Time\\ Machine\\ Backups/Cod-Time-Series-Data/scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count unique loci in genepop file with the 20k cutoff:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15365\r\n"
     ]
    }
   ],
   "source": [
    "!python count_unique_loci_gp.py ../test_20k_cutoff/batch_3_20k.genepop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fst kernel smoothing: off\n",
      "Bootstrap resampling: off\n",
      "Percent samples limit per population: 0.5\n",
      "Locus Population limit: 1\n",
      "Minimum stack depth: 10\n",
      "Log liklihood filtering: off; threshold: 0\n",
      "Minor allele frequency cutoff: 0\n",
      "Maximum observed heterozygosity cutoff: 1\n",
      "Applying Fst correction: none.\n",
      "Parsing population map...\n",
      "Error: Failed to open or parse population map file 'popmap_b3.txt'.\n"
     ]
    }
   ],
   "source": [
    "!populations -b 3 -P test_30k_cutoff -M popmap_b3.txt -t 10 -r 0.50 -p 1 -m 10 --genepop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15389\r\n"
     ]
    }
   ],
   "source": [
    "!python count_unique_loci_gp.py ../test_30k_cutoff/batch_3_30k.genepop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PROBLEM SOLVED\n",
    "\n",
    "I figured it out! When I exclude samples at the 20k cut off, I retain 15365 loci, and when I exclude samples at the 30k cut off, I retain 15389 loci. So I'm going to move forward with the 20k because that's still in the range of what Dan called ideal (20-40k), and there doesn't seem to be a big difference between my two cut-offs. Hooray!\n",
    "\n",
    "It's still a little lower than ideal (compared to Mary who has ~21k loci after ``populations`` with the same species) but I think this is where I move forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
